{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc759772",
   "metadata": {},
   "source": [
    "<div class=\"frontmatter text-center\">\n",
    "<h1>Geospatial Data Science</h1>\n",
    "<h2>Lecture 3: Choropleth Mapping</h2>\n",
    "<h3>IT University of Copenhagen, Spring 2024</h3>\n",
    "<h3>Instructor: Michael Szell</h3>\n",
    "</div>\n",
    "\n",
    "This notebook was adapted from:\n",
    "* A course on Geographic Data Science: https://darribas.org/gds_course/content/bD/lab_D.html\n",
    "\n",
    "In this session, we will build on all we have learnt so far about loading and manipulating (spatial) data and apply it to one of the most commonly used forms of spatial analysis: choropleth maps. These are maps that display the spatial distribution of a variable value, based on:\n",
    "- value **aggegration** with polygons demarcating e.g. regions, municipalities, countries etc.\n",
    "- classification of polygon values using a **classification scheme**\n",
    "- visualization of classes using a in color scheme, also called **palette**. \n",
    "\n",
    "Although there are many ways in which you can classify and visualize your values, we will focus in this context only on a handful of them:\n",
    "\n",
    "* Categorical\n",
    "* Equal interval\n",
    "* Quantiles\n",
    "* Fisher-Jenks\n",
    "* Manual classification\n",
    "\n",
    "You can get a feel for the different classification schemes here: https://mgimond.github.io/Spatial/symbolizing-features.html#an-interactive-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a779e3d-759f-42aa-8c6e-787c9d5df54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd \n",
    "import pandas as pd\n",
    "from pysal.lib import examples\n",
    "import seaborn as sns \n",
    "from pysal.viz import mapclassify\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff7b106",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "### PySal\n",
    "\n",
    "[PySal](https://pysal.org/) is a core library for geospatial analysis. In this course we will both use it for classification, like today, and later for analysis of spatial autocorrelation, etc.\n",
    "\n",
    "### seaborn\n",
    "\n",
    "library for statistical data visualization.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will use a data set on [Mexico GDP per capita](https://geographicdata.science/book/data/mexico/README.html), which we can access as a PySAL example dataset. You can read more about PySAL example datasets [here](https://pysal.org/libpysal/notebooks/examples.html).\n",
    "\n",
    "We can get a short explanation of the dataset through the `explain` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131a71f-11e0-4064-ab96-4b436ecf015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.explain(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b3cd78-e4a8-4702-aa34-f985e2659415",
   "metadata": {},
   "source": [
    "Now, to download it from its remote location, we can use `load_example`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31302e7-4320-46b5-a505-998e6ab29950",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = examples.load_example(\"mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bd96a-079b-4387-ac97-6baaf4c07da0",
   "metadata": {},
   "source": [
    "This will download the data and place it on your home directory. We can inspect the directory where it is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6b5c8-acfe-4c54-9312-37bb16dd2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.get_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305f06c-ddfe-4051-8d2c-fcc9301de93c",
   "metadata": {},
   "source": [
    "For this section, we will read the ESRI shapefile, which we can do by pointing `geopandas.read_file` to the `.shp` file. The utility function `get_path` makes it a bit easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdb394-c7d9-456d-8af1-9795f4d3c970",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex = gpd.read_file(examples.get_path(\"mexicojoin.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b2668-2e6d-4775-8ef1-acc6e117e033",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73883c-2af7-4e78-8dfe-f74185aacbf2",
   "metadata": {},
   "source": [
    "And, from now on, `mex` is a table as we are used to so far in this course:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a9d6e-d970-4626-91e1-68f30ad8e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee87882-2624-4231-abc1-58c2484d48fd",
   "metadata": {},
   "source": [
    "The data however does not include a CRS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120210b9-dbc6-4473-8270-a71fa9965e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc93c8-34ec-4987-a9e2-cd9e2e57e611",
   "metadata": {},
   "source": [
    "To be able to add baselayers, we need to specify one. With a bit of research, we find the data are expressed in longitude and latitude using WGS84, so the CRS we can use is `EPSG:4326`. Let's add it to `mex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b84210-589f-4d87-abd0-e6601b5f6785",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.crs = \"EPSG:4326\"\n",
    "mex.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96012b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f3af57",
   "metadata": {},
   "source": [
    "Now we are ready to map!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd1f3fd-8b10-4c39-aba1-ce291dbcb81b",
   "metadata": {},
   "source": [
    "## Choropleths\n",
    "\n",
    "### Categorical values\n",
    "\n",
    "A choropleth for categorical variables simply assigns a different color to every potential value in the series. The main requirement in this case is then for the **color scheme** to reflect the fact that different values are **not ordered** or follow a particular scale.\n",
    "\n",
    "In Python, creating categorical choropleths is possible with one line of code. To demonstrate this, we can plot the Mexican states and the region each belongs to based on the Mexican Statistics Institute (coded in our table as the `INEGI` variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7d84a-268c-4dda-8022-dba9524ab9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.plot(\n",
    "    column=\"INEGI\", \n",
    "    categorical=True, \n",
    "    legend=True,\n",
    "    #cmap='tab20b' # You can specificy your own colormap - but choose one that matches the type of variable (i.e. categorical/qualitative, sequential, diverging)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6f6647-3d43-4145-b742-f246211b84f8",
   "metadata": {},
   "source": [
    "**Let us stop for a second and note a few crucial aspects:**\n",
    "\n",
    "* Note how we are using the same approach as for basic maps, the command `plot`, but we now need to add the argument `column` to specify which column has the value that we want to map.\n",
    "* Since the variable is categorical we need to make that explicit by setting the argument `categorical` to `True`.\n",
    "* As an optional argument, we can set `legend` to `True` and the resulting figure will include a legend with the names of all the values in the map.\n",
    "* Unless we specify a different colormap, the selected one respects the categorical nature of the data by not implying a gradient or scale but a qualitative structure.\n",
    "\n",
    "Both [Matplotib](https://matplotlib.org/2.0.2/users/colormaps.html) and [Colorbrewer](https://colorbrewer2.org/#type=sequential&scheme=BuGn&n=3) have great guides for choosing colormaps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b81ef4b-009c-40fe-b30a-5785eab08642",
   "metadata": {},
   "source": [
    "### Equal interval\n",
    "\n",
    "If, instead of categorical variables, we want to display the geographical distribution of a continuous phenomenon, we need to select a way to encode each value into a color. One potential solution is applying what is usually called \"equal intervals\". The intuition of this method is to split the *range* of the distribution, the difference between the minimum and maximum value, into equally large segments and to assign a different color to each of them according to a palette that reflects the fact that values are ordered.\n",
    "\n",
    "Creating the choropleth is relatively straightforward in Python. For example, to create an equal interval on the GDP per capita in 2000 (`PCGDP2000`), we can run a similar command as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ca51d-ae60-4a32-8dda-52c72ce960e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"equal_interval\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"fmt\": \"{:.0f}\"},  # Remove decimals in legend\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ff936-0a4a-4d0b-b849-23bc3f08bece",
   "metadata": {},
   "source": [
    "**Pay attention to the key differences:**\n",
    "\n",
    "* Instead of specifyig `categorical` as `True`, we replace it by the argument `scheme`, which we will use for all choropleths that require a continuous classification scheme. In this case, we set it to `equal_interval`.\n",
    "* As above, we set the number of colors to 7. \n",
    "* As optional arguments, we can change the colormap to a yellow to green gradient, which is one of the recommended ones by [ColorBrewer](http://colorbrewer2.org/) for a sequential palette. \n",
    "\n",
    "Now, let's dig a bit deeper into the classification, and how exactly we are encoding values into colors. Each segment, also called bins or buckets, can also be calculated using the library `mapclassify` from the `PySAL` family:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb56276-b637-4add-9042-d4ebd310b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = mapclassify.EqualInterval(mex[\"PCGDP2000\"], k=7)\n",
    "eq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dff59-88f4-4162-b33a-7bc3dd57d936",
   "metadata": {},
   "source": [
    "The only additional argument to pass to `Equal_Interval`, other than the actual variable we would like to classify is the number of segments we want to create, `k`, which we are arbitrarily setting to 7 in this case. This will be the number of colors that will be plotted on the map. Although having several can give more detail, at some point the marginal value of an additional one is fairly limited, given the ability of the brain to tell any differences.\n",
    "\n",
    "Once we have classified the variable, we can check the actual break points where values stop being in one class and become part of the next one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8b1d8-1d18-48b6-a84b-34b5e2d6c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5b09c-51e6-4b14-934d-0557ba9d60f8",
   "metadata": {},
   "source": [
    "The array of breaking points above implies that any value in the variable below 15,207.57 will get the first color in the gradient when mapped, values between 15,207.57 and 21,731.14 the next one, and so on.\n",
    "\n",
    "The key characteristic in equal interval maps is that the bins are allocated based on the magnitude on the values, irrespective of how many obervations fall into each bin as a result of it. In highly skewed distributions, this can result in bins with a large number of observations, while others only have a handful of outliers. This can be seen in the summary table printed out above, where ten states are in the first group, but only one state belong to the one with highest values, and zero states are in the second highest class. This distribution of values in the classes can be represented visually with a kernel density plot where the break points are included as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42074bdc-5888-4dad-9f57-e27869430dcc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(mex[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(mex[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in eq.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cdfb1",
   "metadata": {},
   "source": [
    "Technically speaking, the figure is created by overlaying a KDE plot with vertical bars for each of the break points. This makes much more explicit the issue highlighed by which the first bin contains a large amount of observations while the one with top values only encompasses a handful of them.\n",
    "\n",
    "We can also explore the distribution of values with a traditional histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "ax = sns.histplot(mex[\"PCGDP2000\"], bins=5)\n",
    "# Add rug on horizontal axis\n",
    "sns.rugplot(mex[\"PCGDP2000\"], height=0.05, color=\"red\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df80363-47d7-45d5-975a-de89fd331c8f",
   "metadata": {},
   "source": [
    "We can see that the data are highly skewed, which means that an equal interval classification is not the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca577e-f0fe-4557-b055-1001f3c838f0",
   "metadata": {},
   "source": [
    "### Quantiles\n",
    "\n",
    "One solution to obtain a more balanced classification scheme is using quantiles. This, by definition, assigns the same amount of values to each bin: the entire series is laid out in order and break points are assigned in a way that leaves exactly the same amount of observations between each of them. This \"observation-based\" approach contrasts with the \"value-based\" method of equal intervals and, although it can obscure the magnitude of extreme values, it can be more informative in cases with skewed distributions.\n",
    "\n",
    "The code required to create the choropleth mirrors that needed above for equal intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c4e4e-a11e-40cd-a736-aa4bd3a50f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"fmt\": \"{:.0f}\"}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5aa90-7682-487a-8b28-7d7c23fbaa5b",
   "metadata": {},
   "source": [
    "Note how, in this case, the amount of polygons in each color is by definition much more balanced (almost equal in fact, except for rounding differences). This obscures outlier values, which get blurred by significantly smaller values in the same group, but allows to get more detail in the \"most populated\" part of the distribution, where instead of only white polygons, we can now discern more variability.\n",
    "\n",
    "To get further insight into the quantile classification, let's calculate it with `mapclassify`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f7437-f412-4d75-bdaa-3fa8c42a97d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant7 = mapclassify.Quantiles(mex[\"PCGDP2000\"], k=7)\n",
    "quant7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7edc0c-78dd-4171-9852-ea63cf045e6a",
   "metadata": {},
   "source": [
    "And, similarly, the bins can also be inspected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0122da94-cdf2-4edc-a361-74de5d30c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant7.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94505285-3715-4728-a0f0-27fedac2d1a8",
   "metadata": {},
   "source": [
    "The visualization of the distribution can be generated in a similar way as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6ae6a-43c2-486f-8614-f9dd332730aa",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(mex[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(mex[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in quant7.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a00ffa",
   "metadata": {},
   "source": [
    "The KDE plot makes it clear that the better distribution of values in different classes come at the expense of highly uneven class ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f3748-2ec3-4e4c-9d50-f48194952180",
   "metadata": {},
   "source": [
    "### Fisher-Jenks\n",
    "\n",
    "Equal interval and quantiles are only two examples of very many classification schemes to encode values into colors. Although not all of them are integrated into `GeoPandas`, `PySAL` includes several other classification schemes (for a detailed list, have a look at [mapclassify examples](https://pysal.org/mapclassify/notebooks/01_maximum_breaks.html) or [mapclassify documentation](https://pypi.org/project/mapclassify/)). As an example of a more sophisticated one, let us create a Fisher-Jenks choropleth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92968d2-df0c-42c4-bcef-a39671076cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"fisher_jenks\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"fmt\": \"{:.0f}\"}, \n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapclassify.FisherJenks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a3281-96d1-43da-b21e-8e72af1a0f6a",
   "metadata": {},
   "source": [
    "The same classification can be obtained with a similar approach as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f83dc4-a1b4-493c-a93d-f87e23bfa428",
   "metadata": {},
   "outputs": [],
   "source": [
    "fjenks = mapclassify.FisherJenks(mex[\"PCGDP2000\"], k=7)\n",
    "fjenks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158e564-2ae9-4462-826a-ec19a7a928d7",
   "metadata": {},
   "source": [
    "This methodology aims at minimizing the variance *within* each bin while maximizing that *between* different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f975e58-d14a-43f5-9e4a-89f2de0ce015",
   "metadata": {},
   "outputs": [],
   "source": [
    "fjenks.bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3e79c-05f1-4ec0-9529-a90bd96e34a5",
   "metadata": {},
   "source": [
    "Graphically, we can see how the break points are not equally spaced but are adapting to obtain an optimal grouping of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66462aad-c0c6-4837-bb94-83de4db79ccf",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(mex[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(mex[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in fjenks.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e9b70-59f5-405d-83dd-74fd4d9d3d6f",
   "metadata": {},
   "source": [
    "For example, the bin with highest values covers a much wider span that the one with lowest, because there are fewer states in that value range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f041f",
   "metadata": {},
   "source": [
    "### Manual classification\n",
    "\n",
    "Finally, we can make our own manual classification. This is mostly relevant if there are pre-defined values or classes for the mapped value. For example, official definitions of when a state has a low or high per capita income.\n",
    "\n",
    "To demonstrate, we will just use the median and mean values to create 3 different manual classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d101668",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [17444,20863]\n",
    "\n",
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"user_defined\", # choose 'user_defined'\n",
    "    classification_kwds={'bins':bins}, # provide the bins\n",
    "    cmap=\"YlGn\",\n",
    "    legend=True,\n",
    "    legend_kwds={\"fmt\": \"{:.0f}\"},\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ea9ec",
   "metadata": {},
   "source": [
    "A closer inspection of the distribution of values shows us that only two states have a per capita income between the median and the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e64d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "userdef = mapclassify.UserDefined(mex[\"PCGDP2000\"], bins)\n",
    "userdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef6d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Plot the kernel density estimation (KDE)\n",
    "sns.kdeplot(mex[\"PCGDP2000\"], fill=True)\n",
    "# Add a blue tick for every value at the bottom of the plot (rugs)\n",
    "sns.rugplot(mex[\"PCGDP2000\"], alpha=0.5)\n",
    "# Loop over each break point and plot a vertical red line\n",
    "for cut in userdef.bins:\n",
    "    plt.axvline(cut, color='red', linewidth=0.75)\n",
    "# Display image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e843ec10-1eef-44e4-bd24-eebbc77b11ed",
   "metadata": {},
   "source": [
    "## Zooming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b5467-a00a-4826-8cb8-145e7d6adf75",
   "metadata": {},
   "source": [
    "### Zoom into full map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a282c-a01e-43ab-bed4-bcf6a37ac2f1",
   "metadata": {},
   "source": [
    "A general map of an entire region, or urban area, can sometimes obscure local patterns because they happen at a much smaller scale that cannot be perceived in the global view. One way to solve this is by providing a focus of a smaller part of the map in a separate figure. Although there are many ways to do this in Python, the most straightforward one is to reset the limits of the axes to center them in the area of interest.\n",
    "\n",
    "As an example, let us consider the quantile map produced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb854706-d514-447a-9b8b-6199fc4ec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d484e6-dccc-4896-b39c-e6dd81c25aca",
   "metadata": {},
   "source": [
    "If we want to focus around the capital, Mexico DF, the first step involves realising that such area of the map (the little dark green polygon in the SE centre of the map), falls within the coordinates of -102W/-97W, and 18N/21N, roughly speaking. To display a zoom map into that area, we can do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50e909-e248-46bc-bcb8-62c16792eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure\n",
    "f, ax = plt.subplots(1)\n",
    "# Draw the choropleth\n",
    "mex.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False,\n",
    "    ax=ax\n",
    ")\n",
    "# Redimensionate X and Y axes to desired bounds\n",
    "ax.set_ylim(18, 21)\n",
    "ax.set_xlim(-102, -97);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bee90-9365-445d-9f7e-b1996677b7bd",
   "metadata": {},
   "source": [
    "### Partial map\n",
    "\n",
    "The approach above is straightforward, but not necessarily the most efficient one: note that, to generate a map of a potentially very small area, we effectively draw the entire (potentially very large) map, and discard everything except the section we want. This is not straightforward to notice at first sight, but what Python is doing in the code cell above is plottin the entire `mex` object, and only then focusing the figure on the X and Y ranges specified in `set_xlim`/`set_ylim`.\n",
    "\n",
    "Sometimes, this is required. For example, if we want to retain the same coloring used for the national map, but focus on the region around Mexico DF, this approach is the easiest one.\n",
    "\n",
    "However, sometimes, we only need to plot the _geographical features_ within a given range, and we either don't need to keep the national coloring (e.g. we are using a single color), or we want a classification performed _only_ with the features in the region.\n",
    "\n",
    "For these cases, it is computationally more efficient to select the data we want to plot first, and then display them through `plot`. For this, we can rely on the `cx` operator (coordinate based indexing): https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.cx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d24e6-e148-46a3-8384-1cb64a544a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = mex.cx[-102:-97, 18:21]\n",
    "subset.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffe5dc4-e95f-49d8-91ce-dc2045f9446c",
   "metadata": {},
   "source": [
    "We query the range of spatial coordinates similarly to how we query indices with `loc`. Note however the result includes full geographic features, and hence the polygons with at least some area within the range are included fully. This results in a larger range than originally specified.\n",
    "\n",
    "This approach is a \"spatial slice\", where instead of subsetting by indices of the table, slices are based on the spatial coordinates of the data represented in the table.\n",
    "\n",
    "Since the result is a `GeoDataFrame` itself, we can create a choropleth that is based only on the data in the subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3abb9a-e7c9-4922-b9d5-971225ca60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.plot(\n",
    "    column=\"PCGDP2000\", \n",
    "    scheme=\"quantiles\", \n",
    "    k=7,\n",
    "    cmap=\"YlGn\",\n",
    "    legend=False\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cccd46",
   "metadata": {},
   "source": [
    "## Advanced: Inspection of classification schemes\n",
    "\n",
    "As a special case of clustering, the definition of the number of classes and the class boundaries pose a problem to the map designer. We want to find the 'optimal' class bins, but the optimal depends on the purpuse with the analysis/map!\n",
    "\n",
    "Relevant considerations for the \"optimal\" classfication scheme is both it's ability to show the true distribution of values, comprehensibility, but also the spatial distribution of the attribute values and the ability of the classifier to convey a sense of that spatial distribution. As you will see later on, this is not necessarily directly related to the statistical distribution of the attribute values. We will return to a joint consideration of both the statistical and spatial distribution of the attribute values in comparison of classifiers below.\n",
    "\n",
    "For map classification, a common optimality criterion is a measure of fit. In mapclassify, the “**absolute deviation around class medians” (ADCM)** is calculated and provides a measure of fit that allows for comparison of alternative classifiers for the same value of . The ADCM will give us a sense of how “compact” each group is. To see this, we can compare different classifiers for  on the Mexico data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunch classifier objects\n",
    "classifiers = eq, quant7, fjenks, userdef\n",
    "# Collect ADCM for each classifier\n",
    "fits = np.array([c.adcm for c in classifiers])\n",
    "# Convert ADCM scores to a DataFrame\n",
    "adcms = pd.DataFrame(fits)\n",
    "# Add classifier names\n",
    "adcms[\"classifier\"] = [c.name for c in classifiers]\n",
    "# Add column names to the ADCM\n",
    "adcms.columns = [\"ADCM\", \"Classifier\"]\n",
    "ax = sns.barplot(\n",
    "    y=\"Classifier\", x=\"ADCM\", data=adcms, palette=\"Pastel1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a9715",
   "metadata": {},
   "source": [
    "We can see that our user defined classification is not performing very well. Fisher-Jenks has the best fit (lower is better!), which is not surprising - but equal interval is not too far of.\n",
    "Depending on the audience, an equal interval classification might be the best choice.\n",
    "\n",
    "The ADCM provides a global measure of fit which can be used to compare the alternative classifiers. As a complement to this global perspective, it can be revealing to consider how each of the observations in our data was classified across the alternative approaches. To do this we can add the class bin attribute (yb) generated by the mapclassify classifiers as additional columns in the dataframe to visualise how they map to observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb15231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append class values as a separate column\n",
    "mex[\"Equal Interval\"] = eq.yb\n",
    "mex[\"Quantiles\"] = quant7.yb\n",
    "mex[\"Fisher-Jenks\"] = fjenks.yb\n",
    "mex[\"User Defined\"] = userdef.yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(9, 3))\n",
    "sns.heatmap(\n",
    "    mex.set_index(\"NAME\")\n",
    "    .sort_values(\"PCGDP2000\")[\n",
    "        [\n",
    "            \"Fisher-Jenks\",\n",
    "            \"Equal Interval\",\n",
    "            \"Quantiles\",\n",
    "            \"User Defined\"\n",
    "        ]\n",
    "    ]\n",
    "    .T,\n",
    "    cmap=\"YlGn\",\n",
    "    cbar=False,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"State ID\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
